import { TypeORMVectorStore } from '@langchain/community/vectorstores/typeorm';
import { ChatPromptTemplate } from '@langchain/core/prompts';
import { RunnableSequence } from '@langchain/core/runnables';
import { ChatOllama, OllamaEmbeddings } from '@langchain/ollama';
import { ChatOpenAI, OpenAIEmbeddings } from '@langchain/openai';
import { Injectable } from '@nestjs/common';
import { InjectRepository } from '@nestjs/typeorm';
import { createRetrievalChain } from 'langchain/chains/retrieval';
import { AppDataSource } from 'src/data-source';
import { ChatMessages } from 'src/question-answer/entities/chat-messages.entity';
import { Repository } from 'typeorm';

@Injectable()
export class QuestionAnswerService {
  constructor(
    @InjectRepository(ChatMessages)
    private chatMessagesRepository: Repository<ChatMessages>,
  ) {}

  async questionOllamaGemma3AllMinilm(question: string) {
    const embeddings = new OllamaEmbeddings({ model: 'all-minilm' });
    const vectorStore = await TypeORMVectorStore.fromDataSource(embeddings, {
      postgresConnectionOptions: AppDataSource,
      tableName: 'document_ollama_allminilm',
    });

    const retriever = vectorStore.asRetriever();
    const prompt = ChatPromptTemplate.fromTemplate(`
      Eres un asistente √∫til que responde preguntas basado estrictamente en los documentos proporcionados.

      Contexto:
      {context}

      Pregunta:
      {input}
    `);
    const llm = new ChatOllama({ model: 'gemma3:latest' });

    const chain = await createRetrievalChain({
      combineDocsChain: RunnableSequence.from([prompt, llm]),
      retriever,
    });

    const response = await chain.invoke({ input: question });
    await this.chatMessagesRepository.save([
      {
        role: 'user',
        content: question,
      },
      {
        role: 'assistant',
        content: String(response.answer?.content),
        chatModel: 'Gemma3',
        embeddingModel: 'All-minilm',
      },
    ]);
    return String(response.answer?.content ?? 'No se encontr√≥ respuesta');
  }

  async questionOpenAI35Turbo3Small(question: string) {
    const embeddings = new OpenAIEmbeddings({
      apiKey: process.env.OPENAI_API_KEY,
      model: 'text-embedding-3-small',
    });
    const vectorStore = await TypeORMVectorStore.fromDataSource(embeddings, {
      postgresConnectionOptions: AppDataSource,
      tableName: 'document_openai_3small',
    });

    const retriever = vectorStore.asRetriever();
    const prompt = ChatPromptTemplate.fromTemplate(`
      Eres un asistente √∫til que responde preguntas basado estrictamente en los documentos proporcionados.

      Contexto:
      {context}

      Pregunta:
      {input}
    `);
    const llm = new ChatOpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      model: 'gpt-3.5-turbo',
    });

    const chain = await createRetrievalChain({
      combineDocsChain: RunnableSequence.from([prompt, llm]),
      retriever,
    });

    const response = await chain.invoke({ input: question });
    await this.chatMessagesRepository.save([
      {
        role: 'user',
        content: question,
      },
      {
        role: 'assistant',
        content: String(response.answer?.content),
        chatModel: 'GPT-3.5-turbo',
        embeddingModel: 'Text-embedding-3-small',
      },
    ]);
    return String(response.answer?.content ?? 'No se encontr√≥ respuesta');
  }

  async consultQueryCRMOpenAI35Turbo3Small(question: string) {
    const llm = new ChatOpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      model: 'gpt-3.5-turbo',
    });

    const schema = `
Tablas disponibles:

clientes(id, nombre, email, fecha_registro)
ventas(id, cliente_id, monto, fecha_venta)
productos(id, nombre, precio, stock)
`;

    const prompt = ChatPromptTemplate.fromTemplate(`
Eres un asistente experto en SQL. Tu tarea es generar √∫nicamente la consulta SQL en formato PostgreSQL basada en la siguiente pregunta, usando el esquema de base de datos proporcionado.

üîí No expliques, no agregues ning√∫n comentario. Solo responde con la query lista para ejecutar.

üìö Esquema de base de datos:
{schema}

‚ùì Pregunta:
{input}
`);

    const chain = RunnableSequence.from([prompt, llm]);

    const result = await chain.invoke({
      schema,
      input: question,
    });

    return String(result.content ?? 'No se encontr√≥ respuesta').trim();
  }
}
